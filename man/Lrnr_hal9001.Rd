% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Lrnr_hal9001.R
\docType{class}
\name{Lrnr_hal9001}
\alias{Lrnr_hal9001}
\title{The Scalable Highly Adaptive LASSO}
\format{\code{\link{R6Class}} object.}
\usage{
Lrnr_hal9001
}
\value{
Learner object with methods for training and prediction. See
\code{\link{Lrnr_base}} for documentation on learners.
}
\description{
The Highly Adaptive LASSO is an estimation procedure that generates a design
matrix consisting of basis functions corresponding to covariates and
interactions of covariates and fits LASSO regression to this (usually) very
wide matrix, recovering a nonparametric functional form that describes the
target prediction function as a composition of subset functions with finite
variation norm. This implementation uses the \code{hal9001} R package, which
provides both a custom implementation (based on the \code{origami} package)
of the CV-LASSO as well the standard call to \code{cv.glmnet} from the
\code{glmnet} package.
}
\section{Parameters}{

\describe{
\item{\code{degrees="degrees"}}{ The highest order of interaction terms for
which the basis functions ought to be generated. The default (\code{NULL})
corresponds to generating basis functions for the full dimensionality of
the input matrix.
}
\item{\code{fit_type="fit_type"}}{ The specific routine to be called when
fitting the LASSO regression in a cross-validated manner. Choosing the
\code{glmnet} option will result in a call to \code{cv.glmnet} while
\code{origami} will produce a (faster) call to a custom routine based on
the \code{origami} package.
}
\item{\code{n_folds="n_folds"}}{ Integer for the number of folds to be used
when splitting the data for cross-validation. This defaults to 10 as this
is the convention for v-fold cross-validation.
}
\item{\code{use_min="use_min"}}{ Determines which lambda is selected from
\code{cv.glmnet}. \code{TRUE} corresponds to \code{"lambda.min"} and
\code{FALSE} corresponds to \code{"lambda.1se"}.
}
\item{\code{...}}{ Other parameters passed directly to
\code{\link[hal9001]{fit_hal}}. See its documentation for details.
}
}
}

\seealso{
Other Learners: \code{\link{Custom_chain}},
  \code{\link{Lrnr_HarmonicReg}}, \code{\link{Lrnr_arima}},
  \code{\link{Lrnr_bartMachine}}, \code{\link{Lrnr_base}},
  \code{\link{Lrnr_bilstm}}, \code{\link{Lrnr_condensier}},
  \code{\link{Lrnr_cv}}, \code{\link{Lrnr_dbarts}},
  \code{\link{Lrnr_define_interactions}},
  \code{\link{Lrnr_expSmooth}},
  \code{\link{Lrnr_glm_fast}}, \code{\link{Lrnr_glmnet}},
  \code{\link{Lrnr_glm}}, \code{\link{Lrnr_grf}},
  \code{\link{Lrnr_h2o_grid}},
  \code{\link{Lrnr_independent_binomial}},
  \code{\link{Lrnr_lstm}}, \code{\link{Lrnr_mean}},
  \code{\link{Lrnr_nnls}}, \code{\link{Lrnr_optim}},
  \code{\link{Lrnr_pca}},
  \code{\link{Lrnr_pkg_SuperLearner}},
  \code{\link{Lrnr_randomForest}},
  \code{\link{Lrnr_ranger}}, \code{\link{Lrnr_rpart}},
  \code{\link{Lrnr_rugarch}}, \code{\link{Lrnr_sl}},
  \code{\link{Lrnr_solnp_density}},
  \code{\link{Lrnr_solnp}},
  \code{\link{Lrnr_subset_covariates}},
  \code{\link{Lrnr_svm}}, \code{\link{Lrnr_tsDyn}},
  \code{\link{Lrnr_xgboost}}, \code{\link{Pipeline}},
  \code{\link{Stack}}, \code{\link{define_h2o_X}},
  \code{\link{undocumented_learner}}
}
\keyword{data}
