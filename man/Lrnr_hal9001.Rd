% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Lrnr_hal9001.R
\docType{class}
\name{Lrnr_hal9001}
\alias{Lrnr_hal9001}
\title{The Scalable Highly Adaptive Lasso}
\format{\code{\link{R6Class}} object.}
\value{
Learner object with methods for training and prediction. See
\code{\link{Lrnr_base}} for documentation on learners.
}
\description{
The Highly Adaptive Lasso is an estimation procedure that generates a design
matrix consisting of basis functions corresponding to covariates and
interactions of covariates and fits Lasso regression to this (usually) very
wide matrix, recovering a nonparametric functional form that describes the
target prediction function as a composition of subset functions with finite
variation norm. This implementation uses the \code{hal9001} R package, which
provides both a custom implementation (based on the \code{origami} package)
of the CV-Lasso as well the standard call to \code{cv.glmnet} from the
\code{glmnet} package.
}
\section{Parameters}{

\describe{
\item{\code{max_degree=3}}{ The highest order of interaction
terms for which the basis functions ought to be generated. The default
corresponds to generating basis functions up to all 3-way interactions of
covariates in the input matrix, matching the default in \pkg{hal9001}.
}
\item{\code{fit_type="glmnet"}}{The specific routine to be called when
fitting the Lasso regression in a cross-validated manner. Choosing the
"glmnet" option calls either \code{\link[glmnet]{cv.glmnet}} or
\code{\link[glmnet]{glmnet}}.
}
\item{\code{n_folds=10}}{Integer for the number of folds to be used
when splitting the data for cross-validation. This defaults to 10 as this
is the convention for V-fold cross-validation.
}
\item{\code{use_min=TRUE}}{Determines which lambda is selected from
\code{\link[glmnet]{cv.glmnet}}. \code{TRUE} corresponds to
\code{"lambda.min"} and \code{FALSE} corresponds to \code{"lambda.1se"}.
}
\item{\code{reduce_basis=NULL}}{A \code{numeric} value bounded in the open
interval (0,1) indicating the minimum proportion of ones in a basis
function column needed for the basis function to be included in the
procedure to fit the Lasso. Any basis functions with a lower proportion of
1's than the specified cutoff will be removed. This argument defaults to
\code{NULL}, in which case all basis functions are used in the Lasso stage
of HAL.
}
\item{\code{return_lasso=TRUE}}{A \code{logical} indicating whether or not
to return the \link[glmnet]{\code{glmnet}} fit of the Lasso model.
}
\item{\code{return_x_basis=FALSE}}{A \code{logical} indicating whether or
not to return the matrix of (possibly reduced) basis functions used in the
HAL Lasso fit.
}
\item{\code{basis_list=NULL}}{The full set of basis functions generated
from the input data (from \code{\link[hal9001]{enumerate_basis}}). The
dimensionality of this structure is roughly (n * 2^(d - 1)), where n is
the number of observations and d is the number of columns in the input.
}
\item{\code{cv_select=TRUE}}{A \code{logical} specifying whether the array
of values specified should be passed to \code{\link[glmnet]{cv.glmnet}} in
order to pick the optimal value (based on cross-validation) (when set to
\code{TRUE}) or to simply fit along the sequence of values (or a single
value) using \code{\link[glmnet]{glmnet}} (when set to \code{FALSE}).
}
\item{\code{...}}{ Other parameters passed directly to
\code{\link[hal9001]{fit_hal}}. See its documentation for details.
}
}
}

\seealso{
Other Learners: 
\code{\link{Custom_chain}},
\code{\link{Lrnr_HarmonicReg}},
\code{\link{Lrnr_arima}},
\code{\link{Lrnr_bartMachine}},
\code{\link{Lrnr_base}},
\code{\link{Lrnr_bilstm}},
\code{\link{Lrnr_caret}},
\code{\link{Lrnr_condensier}},
\code{\link{Lrnr_cv}},
\code{\link{Lrnr_dbarts}},
\code{\link{Lrnr_define_interactions}},
\code{\link{Lrnr_density_discretize}},
\code{\link{Lrnr_density_hse}},
\code{\link{Lrnr_density_semiparametric}},
\code{\link{Lrnr_earth}},
\code{\link{Lrnr_expSmooth}},
\code{\link{Lrnr_gam}},
\code{\link{Lrnr_gbm}},
\code{\link{Lrnr_glm_fast}},
\code{\link{Lrnr_glmnet}},
\code{\link{Lrnr_glm}},
\code{\link{Lrnr_grf}},
\code{\link{Lrnr_h2o_grid}},
\code{\link{Lrnr_haldensify}},
\code{\link{Lrnr_independent_binomial}},
\code{\link{Lrnr_lstm}},
\code{\link{Lrnr_mean}},
\code{\link{Lrnr_multivariate}},
\code{\link{Lrnr_nnls}},
\code{\link{Lrnr_optim}},
\code{\link{Lrnr_pca}},
\code{\link{Lrnr_pkg_SuperLearner}},
\code{\link{Lrnr_polspline}},
\code{\link{Lrnr_pooled_hazards}},
\code{\link{Lrnr_randomForest}},
\code{\link{Lrnr_ranger}},
\code{\link{Lrnr_revere_task}},
\code{\link{Lrnr_rfcde}},
\code{\link{Lrnr_rpart}},
\code{\link{Lrnr_rugarch}},
\code{\link{Lrnr_screener_corP}},
\code{\link{Lrnr_screener_corRank}},
\code{\link{Lrnr_screener_randomForest}},
\code{\link{Lrnr_sl}},
\code{\link{Lrnr_solnp_density}},
\code{\link{Lrnr_solnp}},
\code{\link{Lrnr_stratified}},
\code{\link{Lrnr_subset_covariates}},
\code{\link{Lrnr_svm}},
\code{\link{Lrnr_tsDyn}},
\code{\link{Lrnr_xgboost}},
\code{\link{Pipeline}},
\code{\link{Stack}},
\code{\link{define_h2o_X}()},
\code{\link{undocumented_learner}}
}
\concept{Learners}
\keyword{data}
